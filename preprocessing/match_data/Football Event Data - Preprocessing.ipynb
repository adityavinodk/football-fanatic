{
  "metadata": {
    "name": "Football Event Data - Preprocessing",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val filePath \u003d \"/user/avk3358_nyu_edu/project/data\"\nvar matchDF \u003d spark.read.json(filePath)\n                        .withColumn(\"input_file\", input_file_name())\n                        .withColumn(\"match_id\", monotonically_increasing_id())\nz.show(matchDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "matchDF.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "matchDF.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "matchDF.select(\"date\", \"time\").show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dateConvertedDF \u003d matchDF.withColumn(\"date\", to_date(col(\"date\"), \"dd MMM yyyy\"))\n                             .withColumn(\"time\", to_timestamp(col(\"time\"), \"dd MMM yyyy HH:mm\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dateConvertedDF.select(\"date\", \"time\").show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dateConvertedDF.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val trimmedDF \u003d dateConvertedDF.withColumn(\"city\", lower(trim(col(\"city\"))))\n                        .withColumn(\"country\", lower(trim(col(\"country\"))))\n                        .withColumn(\"sport\", lower(trim(col(\"sport\"))))\n                        .withColumn(\"stadium\", lower(trim(col(\"stadium\"))))\n                        .withColumn(\"tournament\", lower(trim(col(\"tournament\"))))\n                        .withColumn(\"input_file\", split(col(\"input_file\"), \"/\").getItem(7))\nval rightDF \u003d trimmedDF.withColumn(\"access_date\", to_date(split(col(\"input_file\"), \"\\\\.\").getItem(0))).drop(\"input_file\")\nz.show(rightDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "rightDF.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val badTeamsDF \u003d rightDF.filter(size(col(\"teams\")) \u003e 2).select(\"teams\")\nz.show(badTeamsDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "badTeamsDF.count"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import scala.collection.mutable.HashSet\nimport scala.collection.mutable.ListBuffer\n \ndef modifyArray(inputArray: Array[String]): Array[String] \u003d {\n    val length \u003d inputArray.size\n    if(length \u003d\u003d 2) return inputArray\n    val table \u003d HashSet[String](\"Slo\", \"Wol\", \"Co\", \"Li\", \"Ala\", \"E\", \"Bayer Le\", \"Le Ha\", \"Ju\", \"GD Cha\")\n    val outBuffer \u003d ListBuffer[String]()\n    var i \u003d 0\n    while (i \u003c length){\n        var s \u003d inputArray(i)\n        if(s.contains(\"-\")){\n            s \u003d inputArray(i).split(\"-\")(1).trim()\n        }\n        if(table.contains(s)){\n            outBuffer +\u003d (inputArray(i)+\"v\"+inputArray(i+1)).toLowerCase()\n            i+\u003d2\n        }\n        else{\n            outBuffer +\u003d inputArray(i).toLowerCase()\n            i+\u003d1\n        }\n    }\n    return outBuffer.toArray\n}\nval modifyTeamsArray \u003d udf(modifyArray _)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val updatedDF \u003d rightDF.withColumn(\"teams\", modifyTeamsArray(col(\"teams\")))\nz.show(updatedDF.select(\"teams\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "updatedDF.filter(size(col(\"teams\")) \u003e 2).count"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "updatedDF.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(updatedDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\nval explodedDF \u003d updatedDF.withColumn(\"ticket\", explode(col(\"tickets\")))\n\nval ticketDF \u003d explodedDF.select(\n  col(\"match_id\"),\n  col(\"access_date\"),\n  col(\"ticket.category\").alias(\"ticket_category\"),\n  col(\"ticket.info\").alias(\"ticket_info\"),\n  col(\"ticket.price\").alias(\"ticket_price\")\n)\n\nz.show(ticketDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "ticketDF.count"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.types.DoubleType\n\nval updatedTicketDF \u003d ticketDF.withColumn(\"ticket_price\", trim(split(col(\"ticket_price\"), \"£\").getItem(1)).cast(DoubleType))\nz.show(updatedTicketDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "updatedTicketDF.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val filteredTicketDF \u003d updatedTicketDF.withColumn(\"ticket_info\", lower(trim(regexp_replace(col(\"ticket_info\"), \"[*#-.:\u003d]\", \"\"))))\n                                .withColumn(\"ticket_category\", lower(trim(col(\"ticket_category\"))))\nz.show(filteredTicketDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val eventOutputPath \u003d \"/user/avk3358_nyu_edu/project/data/cleaned-event-df.parquet\"\n\nupdatedDF.write.mode(\"overwrite\").parquet(eventOutputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val ticketOutputPath \u003d \"/user/avk3358_nyu_edu/project/data/ticket-df.parquet\"\n\nfilteredTicketDF.write.mode(\"overwrite\").parquet(ticketOutputPath)"
    }
  ]
}