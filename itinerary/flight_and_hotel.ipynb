{
  "metadata": {
    "name": "test2",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val matchDataFilePath \u003d \"/user/avk3358_nyu_edu/project/data/cleaned-event-df.parquet\"\nvar matchDF \u003d spark.read.parquet(matchDataFilePath)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(matchDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "matchDF.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val ticketDataFilePath \u003d \"/user/avk3358_nyu_edu/project/data/ticket-df.parquet\"\nvar ticketDF \u003d spark.read.parquet(ticketDataFilePath)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(ticketDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "ticketDF.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def getMatchesByTeams(teams: Array[String], matchesDF: org.apache.spark.sql.DataFrame): org.apache.spark.sql.DataFrame \u003d {\n    matchesDF\n      .filter(array_contains($\"teams\", teams(0)) \u0026\u0026 array_contains($\"teams\", teams(1)))\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val teams \u003d Array(\"Real Madrid\", \"Bayern Munich\")\n\nval filteredMatchesDF \u003d getMatchesByTeams(teams, matchDF)\n\nz.show(filteredMatchesDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val joinedDF \u003d ticketDF\n  .join(filteredMatchesDF, filteredMatchesDF(\"match_id\") \u003d\u003d\u003d ticketDF(\"match_id\"), \"inner\")\n  .select(filteredMatchesDF(\"date\"), filteredMatchesDF(\"teams\"), ticketDF(\"match_id\"), ticketDF(\"ticket_price\"))\n\njoinedDF.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(joinedDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(joinedDF.select($\"match_id\").distinct())"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(joinedDF.groupBy($\"date\", $\"teams\", $\"match_id\").agg(min(\"ticket_price\").alias(\"lowest_ticket_price\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def getMatchDataByTeams(teams: Array[String], matchDF: org.apache.spark.sql.DataFrame, ticketDF: org.apache.spark.sql.DataFrame): org.apache.spark.sql.DataFrame \u003d {\n    val matchData \u003d matchDF\n      .filter(array_contains($\"teams\", teams(0)) \u0026\u0026 array_contains($\"teams\", teams(1)))\n      \n    val allDF \u003d ticketDF\n      .join(matchData, matchData(\"match_id\") \u003d\u003d\u003d ticketDF(\"match_id\"), \"inner\")\n      .select(ticketDF(\"match_id\"), matchData(\"date\"), matchData(\"time\"), matchData(\"city\"), matchData(\"teams\"), ticketDF(\"ticket_price\"))\n      \n    allDF.groupBy($\"date\", $\"teams\", $\"match_id\", $\"time\", $\"city\").agg(min(\"ticket_price\").alias(\"lowest_ticket_price\"))\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val matchCleaned \u003d getMatchDataByTeams(teams, matchDF, ticketDF)\nz.show(matchCleaned)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// part 2: flight data\n\nval airbnbDataDir \u003d \"/user/mjd9571_nyu_edu/project/airbnb_listings_cleaned.csv\"\n\nval flightDataDir \u003d \"/user/zc2398_nyu_edu/flight-data-cleaned/\"\n\nval hotelDataDir \u003d \"/user/yl12081_nyu_edu/project/hotel_data_clean.parquet\"\n\n\nval airbnbDF \u003d spark.read.option(\"header\",\"true\").csv(airbnbDataDir)\nval flightDF \u003d spark.read.parquet(flightDataDir)\nval hotelDF \u003d spark.read.parquet(hotelDataDir)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(flightDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\ndef getAverageFlightPriceDF(inDF:org.apache.spark.sql.DataFrame, flightDF:org.apache.spark.sql.DataFrame) \u003d {\n    val averagePrice \u003d inDF.groupBy(\"arrival_datetime\", \"airline\",\"origin\").agg(round(avg(\"price\"),2).as(\"avg_price\"))\n    val all \u003d inDF.join(averagePrice, Seq(\"arrival_datetime\", \"airline\",\"origin\"),\"left\").drop(\"price\").withColumnRenamed(\"avg_price\",\"price\")\n    // println(all.columns.toList)\n    \n    val allColumnsExceptID \u003d all.columns.filter(_ !\u003d \"id\").toList\n    all.groupBy(allColumnsExceptID.map(col): _*).agg(max(\"id\").as(\"latest_id\"))  // Convert column names to Column type and spread them as arguments\n}\n\n\ndef getFlightDataFromMatch(matchData: org.apache.spark.sql.DataFrame) \u003d {\n    val inboundFlightRaw \u003d flightDF.join(matchData, lower(flightDF(\"destination_city\"))\u003d\u003d\u003dmatchData(\"city\") \u0026\u0026 flightDF(\"arrival_datetime\") \u003c matchData(\"time\").minus(expr(\"interval 5 hours\")) \u0026\u0026 flightDF(\"arrival_datetime\") \u003e matchData(\"time\").minus(expr(\"interval 3 days\"))).select(\"arrival_datetime\", \"airline\",\"id\", \"price\", \"city\",\"match_id\",\"teams\",\"origin_city\",\"lowest_ticket_price\").withColumnRenamed(\"origin_city\",\"origin\")\n    val inboundFlight \u003d getAverageFlightPriceDF(inboundFlightRaw, flightDF).withColumnRenamed(\"latest_id\",\"arrival_flight_id\").withColumnRenamed(\"price\",\"in_price\")\n    \n    // inboundFlight\n    val outboundFlightRaw \u003d flightDF.join(matchData, lower(flightDF(\"origin_city\"))\u003d\u003d\u003dmatchData(\"city\") \u0026\u0026 flightDF(\"arrival_datetime\") \u003e matchData(\"time\").plus(expr(\"interval 5 hours\")) \u0026\u0026 flightDF(\"arrival_datetime\") \u003c matchData(\"time\").plus(expr(\"interval 3 days\"))).select(\"departure_datetime\", \"arrival_datetime\",\"airline\",\"id\", \"price\", \"match_id\",\"destination_city\",\"origin_city\").withColumnRenamed(\"destination_city\",\"return_city\").withColumnRenamed(\"origin_city\",\"origin\")\n    \n    val outboundFlight \u003d getAverageFlightPriceDF(outboundFlightRaw, flightDF).withColumnRenamed(\"latest_id\",\"departure_flight_id\").drop(\"arrival_datetime\",\"origin\").withColumnRenamed(\"airline\",\"airline_out\").withColumnRenamed(\"price\",\"out_price\")\n    \n    \n    inboundFlight.join(outboundFlight, inboundFlight(\"match_id\")\u003d\u003d\u003doutboundFlight(\"match_id\") \u0026\u0026 inboundFlight(\"origin\")\u003d\u003d\u003doutboundFlight(\"return_city\"), \"cross\").withColumn(\"flight_price\", col(\"in_price\")+col(\"out_price\")).drop(\"in_price\",\"out_price\",\"return_city\").withColumnRenamed(\"arrival_datetime\",\"arrival_time\").withColumnRenamed(\"departure_datetime\",\"departure_time\")\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val flightOutput \u003d getFlightDataFromMatch(matchCleaned)\nz.show(flightOutput)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "flightOutput.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val convertedDf \u003d flightOutput\n  .withColumn(\"arrival_date\", $\"arrival_time\".cast(\"date\"))\n  .withColumn(\"departure_date\", $\"departure_time\".cast(\"date\"))\n\n// Drop the original timestamp columns if no longer needed\nval flightOutputDatetype \u003d convertedDf.drop(\"arrival_time\", \"departure_time\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(flightOutputDatetype)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "flightOutputDatetype.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val hotelPath \u003d \"/user/yl12081_nyu_edu/hotel_data_clean_refined.parquet\"\nval hotelDataframe \u003d spark.read.parquet(hotelPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(hotelDataframe)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "hotelDataframe.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.DataFrame\ndef flightJoinWithHotel(flight: DataFrame, hotel: DataFrame): DataFrame \u003d {\n  val result \u003d flight.alias(\"df1\")\n    .join(\n      hotel.alias(\"df2\"),\n      lower(col(\"df1.city\")) \u003d\u003d\u003d lower(col(\"df2.city\")) \u0026\u0026 // Case-insensitive city comparison\n      col(\"df2.check_in_date\").geq(col(\"df1.arrival_date\")) \u0026\u0026 // checkinDate \u003e\u003d arrival_date\n      col(\"df2.check_in_date\").leq(date_add(col(\"df1.arrival_date\"), 1)) \u0026\u0026 // checkinDate \u003c\u003d arrival_date + 1 day\n      col(\"df2.check_out_date\") \u003d\u003d\u003d col(\"df1.departure_date\"), // checkoutDate \u003d\u003d departure_date\n      \"inner\" // Inner join to only keep matching rows\n    )\n    \n  result.withColumnRenamed(\"departure_date\", \"departure_time\")\n        .withColumnRenamed(\"arrival_date\", \"arrival_time\")\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val hotelOutput \u003d flightJoinWithHotel(flightOutputDatetype, hotelDataframe)\nz.show(hotelOutput)"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val refinedHotelOutput \u003d hotelOutput.drop(\"country\",\"hotel_address\",\"avg_review\",\"square_feet\",\"hotel_info_date\",\"hotel_stay_duration\")"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(refinedHotelOutput)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "refinedHotelOutput.count"
    }
  ]
}